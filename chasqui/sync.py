"""Remote synchronization engine for chasqui workflow automation"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../01_sync.ipynb.

# %% auto 0
__all__ = ['SyncConfig', 'sync', 'quick_sync']

# %% ../01_sync.ipynb 3
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
import json
import tempfile
import os

from .database import ChasquiDB
from .ssh import SSHConnection
from .templates import generate_pbs_script_from_job
from .agent import deploy_agent, trigger_agent, parse_agent_log
from .download import download_completed_jobs

from fastcore.basics import patch

# %% ../01_sync.ipynb 6
class SyncConfig:
    """
    Configuration for sync operations.
    
    Example:
        >>> config = SyncConfig(
        ...     remote_host='bebop',
        ...     chasqui_remote_dir='$HOME/chasqui_remote',
        ...     max_queued=40,
        ...     max_running=30
        ... )
    """
    def __init__(
        self,
        remote_host: str = 'bebop',
        chasqui_remote_dir: str = '$HOME/chasqui_remote',
        max_queued: int = 40,
        max_running: int = 30,
        auto_deploy_agent: bool = True,
        download_results: bool = False
    ):
        self.remote_host = remote_host
        self.chasqui_remote_dir = chasqui_remote_dir
        self.max_queued = max_queued
        self.max_running = max_running
        self.auto_deploy_agent = auto_deploy_agent
        self.download_results = download_results

# %% ../01_sync.ipynb 8
def _upload_vasp_inputs(
    ssh: SSHConnection,
    job: Dict[str, Any],
    work_dir: str
) -> None:
    """
    Upload VASP input files to remote work directory.
    
    Args:
        ssh: Active SSH connection
        job: Job dictionary from database
        work_dir: Remote work directory (expanded path)
    """
    local_path = Path(job['local_path']).expanduser()
    
    if not local_path.exists():
        raise FileNotFoundError(f"Local job directory not found: {local_path}")
    
    # Create remote work directory
    ssh.run(f'mkdir -p {work_dir}')
    
    # Upload VASP input files
    for filename in ['POSCAR', 'INCAR', 'KPOINTS', 'POTCAR']:
        local_file = local_path / filename
        if local_file.exists():
            remote_file = f"{work_dir}/{filename}"
            ssh.upload(str(local_file), remote_file)
        # Note: POTCAR might not exist for some test cases

# %% ../01_sync.ipynb 9
def _upload_pbs_script(
    ssh: SSHConnection,
    job: Dict[str, Any],
    work_dir: str,
    waiting_dir: str
) -> str:
    """
    Generate and upload PBS script to waiting directory.
    
    Args:
        ssh: Active SSH connection
        job: Job dictionary from database
        work_dir: Remote work directory (expanded)
        waiting_dir: Remote waiting directory (expanded)
        
    Returns:
        Remote script path
    """
    # Generate PBS script with work_dir
    job_with_workdir = job.copy()
    if job_with_workdir.get('vasp_config'):
        config = json.loads(job_with_workdir['vasp_config'])
        config['remote_work_dir'] = work_dir
        job_with_workdir['vasp_config'] = json.dumps(config)
    else:
        job_with_workdir['vasp_config'] = json.dumps({'remote_work_dir': work_dir})
    
    script = generate_pbs_script_from_job(job_with_workdir)
    
    # Write to temporary local file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as tmp:
        tmp.write(script)
        tmp_path = tmp.name
    
    try:
        # Upload to waiting directory
        remote_script = f"{waiting_dir}/{job['job_id']}.sh"
        ssh.upload(tmp_path, remote_script)
        return remote_script
    finally:
        os.unlink(tmp_path)

# %% ../01_sync.ipynb 10
def _check_completed_jobs(
    ssh: SSHConnection,
    completed_dir: str,
    jobs: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Check for completion flags and parse them.
    
    Args:
        ssh: Active SSH connection
        completed_dir: Remote completed directory (expanded)
        jobs: List of job dictionaries to check
        
    Returns:
        List of completed job updates: [{'job_id': ..., 'status': 'DONE', 'pbs_id': ...}, ...]
    """
    completed = []
    
    for job in jobs:
        job_id = job['job_id']
        flag_file = f"{completed_dir}/{job_id}.flag"
        
        if ssh.exists(flag_file):
            # Read flag content
            flag_content = ssh.run(f'cat {flag_file}')
            lines = flag_content.strip().split('\n')
            
            if len(lines) >= 1:
                flag_status = lines[0].strip()  # DONE or FAIL from flag file
                pbs_id = lines[1].strip() if len(lines) > 1 else None
                
                # Map flag status to database status
                status_map = {
                    'DONE': 'COMPLETED',
                    'FAIL': 'FAILED',
                    'COMPLETED': 'COMPLETED',  # In case flag already uses correct name
                    'FAILED': 'FAILED'
                }
                db_status = status_map.get(flag_status, 'FAILED')  # Default to FAILED if unknown
                
                completed.append({
                    'job_id': job_id,
                    'status': db_status,  # ← FIX: Use mapped status
                    'pbs_id': pbs_id
                })
    
    return completed

# %% ../01_sync.ipynb 12
def sync(
    config: Optional[SyncConfig] = None,
    local_db_path: str = "~/.chasqui/jobs.db",
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    Synchronize local and remote job queues.
    
    This is the main orchestration function that:
    1. Uploads queued jobs to remote
    2. Triggers remote agent
    3. Syncs job status back to local DB
    4. Downloads completed results (optional)
    
    Args:
        config: SyncConfig object (if None, uses defaults)
        local_db_path: Path to local SQLite database
        dry_run: If True, show what would happen without executing
        
    Returns:
        Dictionary with sync statistics:
        {
            'uploaded': 5,
            'submitted': 3,
            'completed': 2,
            'failed': 0,
            'timestamp': '2025-10-28T10:30:00Z'
        }
        
    Example:
        >>> from chasqui.sync import sync, SyncConfig
        >>> config = SyncConfig(remote_host='bebop')
        >>> result = sync(config)
        >>> print(f"Uploaded {result['uploaded']} jobs")
    """

    
    from pathlib import Path
    resolved = Path(local_db_path).expanduser().resolve()
    
    # Use default config if not provided
    if config is None:
        config = SyncConfig()
    
    # Initialize database
    db = ChasquiDB(local_db_path)

    from pathlib import Path
    resolved_path = Path(local_db_path).expanduser().resolve()

    # Ensure database is initialized (auto-init on first use)
    # Check if tables exist rather than trying a query
    import sqlite3
    try:
        with sqlite3.connect(db.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='jobs'")
            if cursor.fetchone() is None:
                db.init_db()  # Only init if tables don't exist
    except Exception as e:
        db.init_db()
    
    # Stats to return
    stats = {
        'uploaded': 0,
        'submitted': 0,
        'completed': 0,
        'failed': 0,
        'downloaded': 0,
        'timestamp': datetime.now().isoformat()
    }
    
    if dry_run:
        print("[DRY RUN MODE]")
        queued_jobs = db.get_jobs_by_state('QUEUED_LOCAL')
        print(f"Would upload {len(queued_jobs)} jobs")
        return stats
    
    # Open SSH connection (single session for all operations)
    with SSHConnection(config.remote_host) as ssh:
        
        # Expand remote paths
        chasqui_dir = ssh.run(f'echo {config.chasqui_remote_dir}').strip()
        waiting_dir = f"{chasqui_dir}/waiting"
        submitted_dir = f"{chasqui_dir}/submitted"
        completed_dir = f"{chasqui_dir}/completed"
        
        # Ensure directories exist
        ssh.run(f'mkdir -p {waiting_dir} {submitted_dir} {completed_dir} {chasqui_dir}/logs')
        
        # Deploy agent if needed
        if config.auto_deploy_agent:
            deploy_agent(
                ssh,
                chasqui_remote_dir=config.chasqui_remote_dir,
                max_queued=config.max_queued,
                max_running=config.max_running
            )
        
        # 1. UPLOAD PHASE: Get QUEUED_LOCAL jobs and upload them
        
        from pathlib import Path
        db_path_resolved = Path(db.db_path).expanduser().resolve()
        
        if db_path_resolved.exists():
            import os
            size = os.path.getsize(db_path_resolved)
        
        queued_jobs = db.get_jobs_by_state('QUEUED_LOCAL')
        
        
        for job in queued_jobs:
            job_id = job['job_id']
            
            # Determine work directory
            if job.get('vasp_config'):
                vasp_config = json.loads(job['vasp_config'])
                work_dir = vasp_config.get('remote_work_dir', f"$HOME/scratch/vasp_jobs/{job_id}")
            else:
                work_dir = f"$HOME/scratch/vasp_jobs/{job_id}"
            
            
            # Expand work_dir
            work_dir = ssh.run(f'echo {work_dir}').strip()
            
            
            try:
                # Upload VASP inputs
                _upload_vasp_inputs(ssh, job, work_dir)
                
                # Generate and upload PBS script
                _upload_pbs_script(ssh, job, work_dir, waiting_dir)
                
                # Update database
                db.update_state(job_id, 'UPLOADED', remote_path=work_dir)
                stats['uploaded'] += 1
                
                
            except Exception as e:
                print(f"Error uploading job {job_id}: {e}")
                import traceback
                traceback.print_exc()  # ADD THIS for full traceback
                continue
        
        
        # 2. TRIGGER AGENT: Submit waiting jobs
        if stats['uploaded'] > 0 or db.get_jobs_by_state('UPLOADED'):
            trigger_agent(ssh, config.chasqui_remote_dir)
        
        # 3. PARSE AGENT LOG: See what was submitted
        try:
            log_content = ssh.run(f'cat {chasqui_dir}/logs/agent.log')
            log_entries = parse_agent_log(log_content)
            
            # Find recent submissions (since last sync)
            last_sync = db.get_last_sync()
            last_sync_time = last_sync['timestamp'] if last_sync else None
            
            for entry in log_entries:
                if entry['action'] == 'AGENT_SUBMIT' and entry.get('status') == 'success':
                    job_id = entry.get('job')
                    pbs_id = entry.get('pbs_id')
                    
                    # Check if this job is in our database
                    job = db.get_job(job_id)
                    if job and job['state'] in ['UPLOADED', 'QUEUED_LOCAL']:
                        db.update_state(job_id, 'SUBMITTED', pbs_id=pbs_id)
                        stats['submitted'] += 1
        except Exception as e:
            print(f"Warning: Could not parse agent log: {e}")
        
        # 4. CHECK COMPLETED JOBS: Look for completion flags
        submitted_jobs = db.get_jobs_by_state('SUBMITTED')
        running_jobs = db.get_jobs_by_state('RUNNING')
        
        completed_updates = _check_completed_jobs(
            ssh,
            completed_dir,
            submitted_jobs + running_jobs
        )
        
        for update in completed_updates:
            db.update_state(
                update['job_id'],
                update['status'],  # DONE or FAIL
                pbs_id=update.get('pbs_id')
            )
            
            if update['status'] == 'DONE':
                stats['completed'] += 1
            else:
                stats['failed'] += 1
        
        # 5. DOWNLOAD RESULTS (optional)
        if config.download_results:
            download_stats = download_completed_jobs(
                db,
                ssh,
                file_list=None,  # Use defaults
                limit=None,  # Download all
                update_db=True  # Update downloaded_at timestamps
            )
            
            stats['downloaded'] = download_stats['total_downloaded']
            
            if download_stats['jobs_processed'] > 0:
                print(f"\n✓ Downloaded {download_stats['total_downloaded']} files "
                      f"from {download_stats['jobs_processed']} jobs")
                
                if download_stats['total_skipped'] > 0:
                    print(f"  (skipped {download_stats['total_skipped']} missing files)")
    
    # Log sync operation
    db.log_sync(
        uploaded=stats['uploaded'],
        submitted=stats['submitted'],
        completed=stats['completed'],
        failed=stats['failed'],
        downloaded=stats.get('downloaded', 0)
    )
    
    return stats

# %% ../01_sync.ipynb 14
def quick_sync(remote_host: str = 'bebop') -> Dict[str, Any]:
    """
    Quick sync with default settings.
    
    Args:
        remote_host: SSH host to connect to
        
    Returns:
        Sync statistics
        
    Example:
        >>> from chasqui.sync import quick_sync
        >>> result = quick_sync('bebop')
        >>> print(result)
    """
    config = SyncConfig(remote_host=remote_host)
    return sync(config)
