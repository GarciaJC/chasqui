{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a400bc-cfc4-4852-bd63-c41e0690fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d22fc7-cc29-4fb9-aafc-4d2e30123cf8",
   "metadata": {},
   "source": [
    "# Sync Operations\n",
    "> Remote synchronization engine for chasqui workflow automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60fcb4-803b-4275-973d-321e65d2ca90",
   "metadata": {},
   "source": [
    "This module handles all communication between local and remote systems:\n",
    "\n",
    "- Upload jobs from local queue to remote waiting directory\n",
    "- Trigger remote agent to process waiting jobs\n",
    "- Parse remote agent logs and update local database\n",
    "- Download completed results\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "**Manual 2FA Requirement:** All remote operations happen in a single SSH session \n",
    "to minimize authentication overhead.\n",
    "\n",
    "**Lightweight Remote State:** Remote side uses append-only log files rather than \n",
    "a database for simplicity and robustness.\n",
    "\n",
    "**Self-Perpetuating Queue:** Completed PBS jobs trigger the agent to submit \n",
    "waiting jobs, eliminating need for cron.\n",
    "\n",
    "## Workflow\n",
    "```\n",
    "1. Get QUEUED_LOCAL jobs from database\n",
    "2. Upload VASP inputs to remote work directories\n",
    "3. Generate and upload PBS scripts to waiting/\n",
    "4. Trigger agent to submit jobs\n",
    "5. Parse agent log to see what was submitted\n",
    "6. Update local database with submission status\n",
    "7. Check for completed jobs (flags)\n",
    "8. Update database and optionally download results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4193274-50a0-422c-8a2f-ac80de2e0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Any\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from chasqui.database import ChasquiDB\n",
    "from chasqui.ssh import SSHConnection\n",
    "from chasqui.templates import generate_pbs_script_from_job\n",
    "from chasqui.agent import deploy_agent, trigger_agent, parse_agent_log\n",
    "\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed07abc-6016-4da1-a679-db2716e7afdb",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "The sync operation follows this flow:\n",
    "```\n",
    "Local DB → SSH Connection → Remote System\n",
    "   ↓                            ↓\n",
    "QUEUED_LOCAL              waiting/*.sh\n",
    "   ↓                            ↓\n",
    "UPLOADED                  agent.sh (triggered)\n",
    "   ↓                            ↓\n",
    "SUBMITTED ←── agent.log ←── PBS Queue\n",
    "   ↓\n",
    "COMPLETED\n",
    "```\n",
    "\n",
    "The `sync()` function orchestrates all operations in a single SSH session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102364ce-18a5-4155-947d-463c3ab442bf",
   "metadata": {},
   "source": [
    "## Sync Configuration\n",
    "\n",
    "Store sync parameters in a configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec207afa-74a5-4b0d-8205-2e6ae4398899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SyncConfig:\n",
    "    \"\"\"\n",
    "    Configuration for sync operations.\n",
    "    \n",
    "    Example:\n",
    "        >>> config = SyncConfig(\n",
    "        ...     remote_host='bebop',\n",
    "        ...     chasqui_remote_dir='$HOME/chasqui_remote',\n",
    "        ...     max_queued=40,\n",
    "        ...     max_running=30\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        remote_host: str = 'bebop',\n",
    "        chasqui_remote_dir: str = '$HOME/chasqui_remote',\n",
    "        max_queued: int = 40,\n",
    "        max_running: int = 30,\n",
    "        auto_deploy_agent: bool = True,\n",
    "        download_results: bool = False\n",
    "    ):\n",
    "        self.remote_host = remote_host\n",
    "        self.chasqui_remote_dir = chasqui_remote_dir\n",
    "        self.max_queued = max_queued\n",
    "        self.max_running = max_running\n",
    "        self.auto_deploy_agent = auto_deploy_agent\n",
    "        self.download_results = download_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe971c-93e5-4923-925e-0b43bcdb3ebb",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Internal functions for sync operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8195ca2c-0ebd-4004-b735-aa9ed986dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _upload_vasp_inputs(\n",
    "    ssh: SSHConnection,\n",
    "    job: Dict[str, Any],\n",
    "    work_dir: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload VASP input files to remote work directory.\n",
    "    \n",
    "    Args:\n",
    "        ssh: Active SSH connection\n",
    "        job: Job dictionary from database\n",
    "        work_dir: Remote work directory (expanded path)\n",
    "    \"\"\"\n",
    "    local_path = Path(job['local_path']).expanduser()\n",
    "    \n",
    "    if not local_path.exists():\n",
    "        raise FileNotFoundError(f\"Local job directory not found: {local_path}\")\n",
    "    \n",
    "    # Create remote work directory\n",
    "    ssh.run(f'mkdir -p {work_dir}')\n",
    "    \n",
    "    # Upload VASP input files\n",
    "    for filename in ['POSCAR', 'INCAR', 'KPOINTS', 'POTCAR']:\n",
    "        local_file = local_path / filename\n",
    "        if local_file.exists():\n",
    "            remote_file = f\"{work_dir}/{filename}\"\n",
    "            ssh.upload(str(local_file), remote_file)\n",
    "        # Note: POTCAR might not exist for some test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b3a7de-5930-449a-af16-32060f751e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _upload_pbs_script(\n",
    "    ssh: SSHConnection,\n",
    "    job: Dict[str, Any],\n",
    "    work_dir: str,\n",
    "    waiting_dir: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate and upload PBS script to waiting directory.\n",
    "    \n",
    "    Args:\n",
    "        ssh: Active SSH connection\n",
    "        job: Job dictionary from database\n",
    "        work_dir: Remote work directory (expanded)\n",
    "        waiting_dir: Remote waiting directory (expanded)\n",
    "        \n",
    "    Returns:\n",
    "        Remote script path\n",
    "    \"\"\"\n",
    "    # Generate PBS script with work_dir\n",
    "    job_with_workdir = job.copy()\n",
    "    if job_with_workdir.get('vasp_config'):\n",
    "        config = json.loads(job_with_workdir['vasp_config'])\n",
    "        config['remote_work_dir'] = work_dir\n",
    "        job_with_workdir['vasp_config'] = json.dumps(config)\n",
    "    else:\n",
    "        job_with_workdir['vasp_config'] = json.dumps({'remote_work_dir': work_dir})\n",
    "    \n",
    "    script = generate_pbs_script_from_job(job_with_workdir)\n",
    "    \n",
    "    # Write to temporary local file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as tmp:\n",
    "        tmp.write(script)\n",
    "        tmp_path = tmp.name\n",
    "    \n",
    "    try:\n",
    "        # Upload to waiting directory\n",
    "        remote_script = f\"{waiting_dir}/{job['job_id']}.sh\"\n",
    "        ssh.upload(tmp_path, remote_script)\n",
    "        return remote_script\n",
    "    finally:\n",
    "        os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dac780c-bbeb-4797-938a-0c72191b4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _check_completed_jobs(\n",
    "    ssh: SSHConnection,\n",
    "    completed_dir: str,\n",
    "    jobs: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Check for completion flags and parse them.\n",
    "    \n",
    "    Args:\n",
    "        ssh: Active SSH connection\n",
    "        completed_dir: Remote completed directory (expanded)\n",
    "        jobs: List of job dictionaries to check\n",
    "        \n",
    "    Returns:\n",
    "        List of completed job updates: [{'job_id': ..., 'status': 'DONE', 'pbs_id': ...}, ...]\n",
    "    \"\"\"\n",
    "    completed = []\n",
    "    \n",
    "    for job in jobs:\n",
    "        job_id = job['job_id']\n",
    "        flag_file = f\"{completed_dir}/{job_id}.flag\"\n",
    "        \n",
    "        if ssh.exists(flag_file):\n",
    "            # Read flag content\n",
    "            flag_content = ssh.run(f'cat {flag_file}')\n",
    "            lines = flag_content.strip().split('\\n')\n",
    "            \n",
    "            if len(lines) >= 1:\n",
    "                status = lines[0].strip()  # DONE or FAIL\n",
    "                pbs_id = lines[1].strip() if len(lines) > 1 else None\n",
    "                \n",
    "                completed.append({\n",
    "                    'job_id': job_id,\n",
    "                    'status': status,\n",
    "                    'pbs_id': pbs_id\n",
    "                })\n",
    "    \n",
    "    return completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a18956-6109-44bc-b424-db7e85b44860",
   "metadata": {},
   "source": [
    "## Main Sync Function\n",
    "\n",
    "The orchestrator that performs all sync operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0ddc414-ef59-47c4-b7e1-643446d92b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sync(\n",
    "    config: Optional[SyncConfig] = None,\n",
    "    local_db_path: str = \"$HOME/.chasqui/jobs.db\",\n",
    "    dry_run: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Synchronize local and remote job queues.\n",
    "    \n",
    "    This is the main orchestration function that:\n",
    "    1. Uploads queued jobs to remote\n",
    "    2. Triggers remote agent\n",
    "    3. Syncs job status back to local DB\n",
    "    4. Downloads completed results (optional)\n",
    "    \n",
    "    Args:\n",
    "        config: SyncConfig object (if None, uses defaults)\n",
    "        local_db_path: Path to local SQLite database\n",
    "        dry_run: If True, show what would happen without executing\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with sync statistics:\n",
    "        {\n",
    "            'uploaded': 5,\n",
    "            'submitted': 3,\n",
    "            'completed': 2,\n",
    "            'failed': 0,\n",
    "            'timestamp': '2025-10-28T10:30:00Z'\n",
    "        }\n",
    "        \n",
    "    Example:\n",
    "        >>> from chasqui.sync import sync, SyncConfig\n",
    "        >>> config = SyncConfig(remote_host='bebop')\n",
    "        >>> result = sync(config)\n",
    "        >>> print(f\"Uploaded {result['uploaded']} jobs\")\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = SyncConfig()\n",
    "    \n",
    "    # Initialize database\n",
    "    db = ChasquiDB(local_db_path)\n",
    "\n",
    "    # Ensure database is initialized\n",
    "    try:\n",
    "        db.get_jobs_by_state('QUEUED_LOCAL')  # Test if tables exist\n",
    "    except Exception:\n",
    "        db.init_db()  # Initialize if tables don't exist\n",
    "    \n",
    "    # Stats to return\n",
    "    stats = {\n",
    "        'uploaded': 0,\n",
    "        'submitted': 0,\n",
    "        'completed': 0,\n",
    "        'failed': 0,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    if dry_run:\n",
    "        print(\"[DRY RUN MODE]\")\n",
    "        queued_jobs = db.get_jobs_by_state('QUEUED_LOCAL')\n",
    "        print(f\"Would upload {len(queued_jobs)} jobs\")\n",
    "        return stats\n",
    "    \n",
    "    # Open SSH connection (single session for all operations)\n",
    "    with SSHConnection(config.remote_host) as ssh:\n",
    "        \n",
    "        # Expand remote paths\n",
    "        chasqui_dir = ssh.run(f'echo {config.chasqui_remote_dir}').strip()\n",
    "        waiting_dir = f\"{chasqui_dir}/waiting\"\n",
    "        submitted_dir = f\"{chasqui_dir}/submitted\"\n",
    "        completed_dir = f\"{chasqui_dir}/completed\"\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        ssh.run(f'mkdir -p {waiting_dir} {submitted_dir} {completed_dir} {chasqui_dir}/logs')\n",
    "        \n",
    "        # Deploy agent if needed\n",
    "        if config.auto_deploy_agent:\n",
    "            deploy_agent(\n",
    "                ssh,\n",
    "                chasqui_remote_dir=config.chasqui_remote_dir,\n",
    "                max_queued=config.max_queued,\n",
    "                max_running=config.max_running\n",
    "            )\n",
    "        \n",
    "        # 1. UPLOAD PHASE: Get QUEUED_LOCAL jobs and upload them\n",
    "        queued_jobs = db.get_jobs_by_state('QUEUED_LOCAL')\n",
    "        \n",
    "        for job in queued_jobs:\n",
    "            job_id = job['job_id']\n",
    "            \n",
    "            # Determine work directory\n",
    "            if job.get('vasp_config'):\n",
    "                vasp_config = json.loads(job['vasp_config'])\n",
    "                work_dir = vasp_config.get('remote_work_dir', f\"$HOME/scratch/vasp_jobs/{job_id}\")\n",
    "            else:\n",
    "                work_dir = f\"$HOME/scratch/vasp_jobs/{job_id}\"\n",
    "            \n",
    "            # Expand work_dir\n",
    "            work_dir = ssh.run(f'echo {work_dir}').strip()\n",
    "            \n",
    "            try:\n",
    "                # Upload VASP inputs\n",
    "                _upload_vasp_inputs(ssh, job, work_dir)\n",
    "                \n",
    "                # Generate and upload PBS script\n",
    "                _upload_pbs_script(ssh, job, work_dir, waiting_dir)\n",
    "                \n",
    "                # Update database\n",
    "                db.update_state(job_id, 'UPLOADED', remote_path=work_dir)\n",
    "                stats['uploaded'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading job {job_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 2. TRIGGER AGENT: Submit waiting jobs\n",
    "        if stats['uploaded'] > 0 or db.get_jobs_by_state('UPLOADED'):\n",
    "            trigger_agent(ssh, config.chasqui_remote_dir)\n",
    "        \n",
    "        # 3. PARSE AGENT LOG: See what was submitted\n",
    "        try:\n",
    "            log_content = ssh.run(f'cat {chasqui_dir}/logs/agent.log')\n",
    "            log_entries = parse_agent_log(log_content)\n",
    "            \n",
    "            # Find recent submissions (since last sync)\n",
    "            last_sync = db.get_last_sync()\n",
    "            last_sync_time = last_sync['timestamp'] if last_sync else None\n",
    "            \n",
    "            for entry in log_entries:\n",
    "                if entry['action'] == 'AGENT_SUBMIT' and entry.get('status') == 'success':\n",
    "                    job_id = entry.get('job')\n",
    "                    pbs_id = entry.get('pbs_id')\n",
    "                    \n",
    "                    # Check if this job is in our database\n",
    "                    job = db.get_job(job_id)\n",
    "                    if job and job['state'] in ['UPLOADED', 'QUEUED_LOCAL']:\n",
    "                        db.update_state(job_id, 'SUBMITTED', pbs_id=pbs_id)\n",
    "                        stats['submitted'] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not parse agent log: {e}\")\n",
    "        \n",
    "        # 4. CHECK COMPLETED JOBS: Look for completion flags\n",
    "        submitted_jobs = db.get_jobs_by_state('SUBMITTED')\n",
    "        running_jobs = db.get_jobs_by_state('RUNNING')\n",
    "        \n",
    "        completed_updates = _check_completed_jobs(\n",
    "            ssh,\n",
    "            completed_dir,\n",
    "            submitted_jobs + running_jobs\n",
    "        )\n",
    "        \n",
    "        for update in completed_updates:\n",
    "            db.update_state(\n",
    "                update['job_id'],\n",
    "                update['status'],  # DONE or FAIL\n",
    "                pbs_id=update.get('pbs_id')\n",
    "            )\n",
    "            \n",
    "            if update['status'] == 'DONE':\n",
    "                stats['completed'] += 1\n",
    "            else:\n",
    "                stats['failed'] += 1\n",
    "        \n",
    "        # 5. DOWNLOAD RESULTS (optional)\n",
    "        if config.download_results:\n",
    "            # TODO: Implement result download\n",
    "            pass\n",
    "    \n",
    "    # Log sync operation\n",
    "    db.log_sync(\n",
    "        uploaded=stats['uploaded'],\n",
    "        submitted=stats['submitted'],\n",
    "        completed=stats['completed'],\n",
    "        failed=stats['failed']\n",
    "    )\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d077c90-1cbe-450d-bb9f-3a117c43cb87",
   "metadata": {},
   "source": [
    "## Convenience Functions\n",
    "\n",
    "Simplified interfaces for common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "661dae70-82ca-4019-8dd0-cb3889cb7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def quick_sync(remote_host: str = 'bebop') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Quick sync with default settings.\n",
    "    \n",
    "    Args:\n",
    "        remote_host: SSH host to connect to\n",
    "        \n",
    "    Returns:\n",
    "        Sync statistics\n",
    "        \n",
    "    Example:\n",
    "        >>> from chasqui.sync import quick_sync\n",
    "        >>> result = quick_sync('bebop')\n",
    "        >>> print(result)\n",
    "    \"\"\"\n",
    "    config = SyncConfig(remote_host=remote_host)\n",
    "    return sync(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69a0b8-4177-4f38-9a3e-03933aeab298",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Basic functionality tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee5a1e9f-e4ac-4250-988c-e685d9c62196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SyncConfig works\n",
      "[DRY RUN MODE]\n",
      "Would upload 0 jobs\n",
      "✓ Dry run works\n",
      "\n",
      "✅ All sync module tests passed!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Test SyncConfig\n",
    "config = SyncConfig(\n",
    "    remote_host='test_host',\n",
    "    max_queued=50,\n",
    "    max_running=25\n",
    ")\n",
    "\n",
    "assert config.remote_host == 'test_host'\n",
    "assert config.max_queued == 50\n",
    "assert config.max_running == 25\n",
    "print(\"✓ SyncConfig works\")\n",
    "\n",
    "# Test dry run with temporary database\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.db') as tmp:\n",
    "    test_db_path = tmp.name\n",
    "\n",
    "try:\n",
    "    # Initialize database\n",
    "    db = ChasquiDB(test_db_path)\n",
    "    db.init_db()\n",
    "    \n",
    "    # Test dry run\n",
    "    result = sync(config, local_db_path=test_db_path, dry_run=True)\n",
    "    assert 'uploaded' in result\n",
    "    assert 'timestamp' in result\n",
    "    assert result['uploaded'] == 0  # No jobs in test DB\n",
    "    print(\"✓ Dry run works\")\n",
    "    \n",
    "finally:\n",
    "    # Cleanup\n",
    "    os.unlink(test_db_path)\n",
    "\n",
    "print(\"\\n✅ All sync module tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf1644-5281-4b43-a4b1-5b3b12f44915",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "### Basic Sync\n",
    "```python\n",
    "from chasqui.sync import sync, SyncConfig\n",
    "\n",
    "# Configure\n",
    "config = SyncConfig(\n",
    "    remote_host='bebop',\n",
    "    max_queued=40,\n",
    "    max_running=30\n",
    ")\n",
    "\n",
    "# Sync\n",
    "result = sync(config)\n",
    "\n",
    "print(f\"Uploaded: {result['uploaded']}\")\n",
    "print(f\"Submitted: {result['submitted']}\")\n",
    "print(f\"Completed: {result['completed']}\")\n",
    "print(f\"Failed: {result['failed']}\")\n",
    "```\n",
    "\n",
    "### Quick Sync (with defaults)\n",
    "```python\n",
    "from chasqui.sync import quick_sync\n",
    "\n",
    "result = quick_sync('bebop')\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Complete Workflow Example\n",
    "```python\n",
    "from chasqui.database import ChasquiDB\n",
    "from chasqui.sync import sync, SyncConfig\n",
    "\n",
    "# 1. Create jobs locally\n",
    "db = ChasquiDB()\n",
    "db.init_db()\n",
    "\n",
    "job_id = db.create_job(\n",
    "    local_path=\"/path/to/vasp_job\",\n",
    "    vasp_config={\n",
    "        \"job_name\": \"Au_relax\",\n",
    "        \"cores\": 2,\n",
    "        \"walltime\": \"24:00:00\",\n",
    "        \"project\": \"AARC1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Queue for submission\n",
    "db.update_state(job_id, 'QUEUED_LOCAL')\n",
    "\n",
    "# 3. Sync (uploads, submits, checks status)\n",
    "config = SyncConfig(remote_host='bebop')\n",
    "result = sync(config)\n",
    "\n",
    "print(f\"Job {job_id} uploaded and submitted!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537cc80-8991-48a8-8474-3c2188d4225c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
